{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time, copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "X_train = pd.read_csv(Path('cleaned_data','X_train.csv'), index_col=0)\n",
    "X_val = pd.read_csv(Path('cleaned_data','X_val.csv'), index_col=0)\n",
    "X_test = pd.read_csv(Path('cleaned_data','X_test.csv'), index_col=0)\n",
    "\n",
    "y_train = pd.read_csv(Path('cleaned_data','y_train.csv'), index_col=0)\n",
    "y_val = pd.read_csv(Path('cleaned_data','y_val.csv'), index_col=0)\n",
    "y_test = pd.read_csv(Path('cleaned_data','y_test.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "\n",
    "class MimicIvDataset(Dataset):\n",
    "    \"\"\"MIMIC IV dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file_X, csv_file_y):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "        \"\"\"\n",
    "        self.mimic_df_X = pd.read_csv(Path(csv_file_X), index_col=0)\n",
    "        self.mimic_df_y = pd.read_csv(Path(csv_file_y), index_col=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mimic_df_X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        inputs = torch.tensor(self.mimic_df_X.iloc[idx], dtype=torch.float64)\n",
    "        labels = torch.tensor(self.mimic_df_y.iloc[idx], dtype=torch.float64)\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "train_dataset = MimicIvDataset(csv_file_X=\"cleaned_data/X_train.csv\", csv_file_y=\"cleaned_data/y_train.csv\")\n",
    "val_dataset = MimicIvDataset(csv_file_X=\"cleaned_data/X_val.csv\", csv_file_y=\"cleaned_data/y_val.csv\")\n",
    "test_dataset = MimicIvDataset(csv_file_X=\"cleaned_data/X_test.csv\", csv_file_y=\"cleaned_data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sizes = {'train': 344320, 'val': 38258, 'test': 42509}\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "dataloaders = {'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "               'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=True),\n",
    "               'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=True)}\n",
    "\n",
    "dataset_sizes = {'train': len(train_dataset),\n",
    "                 'val': len(val_dataset),\n",
    "                 'test': len(test_dataset)}\n",
    "print(f'dataset_sizes = {dataset_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MimicAdmissionClassifier(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=67, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.5, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 67\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 128\n",
    "hidden_size3 = 128\n",
    "num_classes = 1\n",
    "dropout_rate = 0.5  # Adjust the dropout rate for regularization\n",
    "\n",
    "# External training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Revised model with flexible activation and data types\n",
    "class MimicAdmissionClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes, dropout_rate, activation_fn=nn.ReLU, dtype=torch.float32):\n",
    "        \"\"\"\n",
    "        A customizable multi-layer perceptron (MLP) model with dropout for classification tasks.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input features.\n",
    "            hidden_size1 (int): Number of units in the first hidden layer.\n",
    "            hidden_size2 (int): Number of units in the second hidden layer.\n",
    "            hidden_size3 (int): Number of units in the third hidden layer.\n",
    "            num_classes (int): Number of output classes (1 for binary classification).\n",
    "            dropout_rate (float): Dropout rate to use between layers (0 for no dropout).\n",
    "            activation_fn (torch.nn.Module): Activation function (default is ReLU).\n",
    "            dtype (torch.dtype): Data type for the model (default is torch.float32).\n",
    "        \"\"\"\n",
    "        super(MimicAdmissionClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer to prevent overfitting\n",
    "        \n",
    "        # Define the sequence of layers with dynamic activation function\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1, dtype=dtype),\n",
    "            activation_fn(),\n",
    "            self.dropout,\n",
    "            nn.Linear(hidden_size1, hidden_size2, dtype=dtype),\n",
    "            activation_fn(),\n",
    "            self.dropout,\n",
    "            nn.Linear(hidden_size2, hidden_size3, dtype=dtype),\n",
    "            activation_fn(),\n",
    "            self.dropout,\n",
    "            nn.Linear(hidden_size3, num_classes, dtype=dtype),\n",
    "            nn.Sigmoid()  # For binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input data tensor of shape (batch_size, input_size).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output prediction tensor of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "# Example instantiation of the model\n",
    "mimic_admission_classifier = MimicAdmissionClassifier(\n",
    "    input_size=input_size,\n",
    "    hidden_size1=hidden_size1,\n",
    "    hidden_size2=hidden_size2,\n",
    "    hidden_size3=hidden_size3,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    activation_fn=nn.ReLU,  # Can be changed to other activations like nn.LeakyReLU\n",
    "    dtype=torch.float64  # Data type can be changed if needed\n",
    ").to(device)\n",
    "\n",
    "# Print model summary (optional)\n",
    "print(mimic_admission_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # keep the best weights stored separately\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    # Each epoch has a training, validation, and test phase\n",
    "    phases = ['train', 'val']\n",
    "    \n",
    "    # Keep track of how loss and accuracy evolves during training\n",
    "    training_curves = {}\n",
    "    for phase in phases:\n",
    "        training_curves[phase+'_loss'] = []\n",
    "        training_curves[phase+'_acc'] = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # This ensures all of our datapoints are flattened\n",
    "                # before feeding them to our model\n",
    "                inputs = inputs.view(inputs.shape[0],-1)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + update weights only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(predictions == labels.data)\n",
    " \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            training_curves[phase+'_loss'].append(epoch_loss)\n",
    "            training_curves[phase+'_acc'].append(epoch_acc)\n",
    "\n",
    "            print(f'{phase:5} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model if it's the best accuracy (based on validation)\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_epoch = epoch\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f} at epoch {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, training_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinicius\\AppData\\Local\\Temp\\ipykernel_29824\\1375628816.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  inputs = torch.tensor(self.mimic_df_X.iloc[idx], dtype=torch.float64)\n",
      "C:\\Users\\Vinicius\\AppData\\Local\\Temp\\ipykernel_29824\\1375628816.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  labels = torch.tensor(self.mimic_df_y.iloc[idx], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5140 Acc: 52.2381\n",
      "val   Loss: 0.5050 Acc: 52.3356\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# loss and optimizer\n",
    "criterion = nn.BCELoss() # BCELoss for binary classification\n",
    "optimizer = torch.optim.Adam(mimic_admission_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Train the model. We also will store the results of training to visualize\n",
    "mimic_admission_classifier, training_curves = train_model(mimic_admission_classifier, dataloaders, dataset_sizes, criterion, optimizer, scheduler, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
